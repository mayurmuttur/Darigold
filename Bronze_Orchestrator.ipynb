{"cells":[{"cell_type":"markdown","source":["### 📘 **Bronze_Orchestrator – RAW → BRONZE Ingestion (Synapse/Fabric)**\n","---\n","\n","#### 🎯 **Purpose**\n","This notebook ingests **RAW data** into a **Bronze Lakehouse layer** using *PySpark*.  \n","- 📂 Supports **CSV** and **ZIP** inputs  \n","- 🧩 Derives schema and row counts  \n","- 🗄 Archives RAW files after ingestion  \n","- 📝 Stages metadata into **`stage.ObjectList`** for lineage/control  \n","- 🔑 Optional **JDBC read** from *Azure SQL* via **Key Vault**\n","\n","---\n","\n","#### ✅ **Prerequisites**\n","- Synapse or Fabric workspace with **PySpark kernel** and **Lakehouse attached**  \n","- Access to **OneLake Files** paths (*RAW*, *Bronze*, *Archive*)  \n","- If JDBC logging is used:  \n","  - Azure SQL database  \n","  - Secrets stored in **Azure Key Vault** (server, database, user, password)  \n","- Proper **Spark pool** or Fabric runtime permissions  \n","\n","---\n","\n","#### ⚙️ **Key Parameters** *(cells 1–3)*\n","- **`SRCDB_NAME`** → Logical source DB name *(e.g., `OTM`)*  \n","- **`PARALLELISM`** → Number of Spark threads/partitions  \n","- **`INCLUDE_CSV` / `INCLUDE_ZIP`** → Enable or disable ingestion formats  \n","\n","---\n","\n","#### 📂 **Derived / Runtime Variables**\n","- **`TMP_EXTRACT_DIR`** → Temp folder for extracted ZIPs  \n","- **`DEFAULT_SCHEMA`** → *`<srcdb>silver`* (adjust if needed)  \n","- **`RUN_DATETIME_UTC` / `RUN_ISO`** → Execution timestamp (*watermark*)  \n","- **`DELTA_TABLE_NAME`** → *`stage.ObjectList`*  \n","\n","---\n","\n","#### 📥 **Inputs**\n","- Files under **RAW** in Lakehouse  \n","  Example:  abfss://Dev_workspace@onelake.dfs.fabric.microsoft.com/BronzeSilver_Dev.Lakehouse/Files\n","\n","- Optional **JDBC query** from *Azure SQL*\n","\n","---\n","\n","#### 📤 **Outputs**\n","- **Bronze Files** → partitioned CSVs stored in `/bronze/<SRCDB_NAME>/`  \n","- **Metadata** → rows written into **`stage.ObjectList`** (*Delta* or *SQL staging table*)  \n","\n","---\n","\n","#### 🛠 **Configuration Tips**\n","- Adjust **`DEFAULT_SCHEMA`** and **`DELTA_TABLE_NAME`** to match naming standards  \n","- Update **`TMP_EXTRACT_DIR`** if RAW path differs  \n","- Use **Key Vault** or *Managed Identity* for secrets — *avoid hardcoding*  \n","\n","---\n","\n","#### ⚠️ **Limitations & Notes**\n","- No widgets → parameters must be edited directly in the first cells  \n","- Not using **Autoloader/DLT** → ingestion is *batch-based Spark*  \n","- Handle **schema drift** for varying CSVs  \n","- ZIP extraction can be slow for very large files  \n","- Ensure Delta writes if you require **ACID merges**  \n","\n","---\n","\n","#### 🔍 **Validation Checklist**\n","- ✅ New files appear in **Bronze path**  \n","- ✅ **`stage.ObjectList`** contains rows for ingested objects  \n","- ✅ If JDBC used → row counts align between **source** and **Lakehouse**\n"],"metadata":{},"id":"cb69e0ba-c0d8-45ae-b3ae-5f0b1baa462e"},{"cell_type":"markdown","source":["## 1) Parameters"],"metadata":{},"id":"a4959e35-a33a-4c42-9829-e8b0ff3b9846"},{"cell_type":"code","source":["SRCDB_NAME = \"OTM\"\n","PARALLELISM = 6\n","INCLUDE_CSV = True\n","INCLUDE_ZIP = True"],"outputs":[],"execution_count":null,"metadata":{"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09c97908-513e-41ba-bb04-1a514aa5a5f3"},{"cell_type":"code","source":["from datetime import datetime, timezone\n","\n","RUNDATE = datetime.utcnow() #.strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n","\n","RUNDATE = str(datetime.now()).replace(\" \", \"_\") \n","\n","# --- Lakehouse source locations (adjust) ---\n","\n","ABFSS_PATH = \"abfss://Dev_workspace@onelake.dfs.fabric.microsoft.com/BronzeSilver.Lakehouse/Files\" \n","RAW_PATH = f\"{ABFSS_PATH}/RAW/{SRCDB_NAME}\"\n","BRONZE_PATH = f\"{ABFSS_PATH}/bronze/{SRCDB_NAME}\"\n","ARCHIVE_PATH = f\"{ABFSS_PATH}/RAW/Archive/{SRCDB_NAME}/{RUNDATE}/\"\n","\n","# Include subdirectories recursively\n","RECURSIVE = False\n","\n","# CSV options\n","CSV_HEADER = True\n","CSV_DELIMITER = ','\n","CSV_QUOTE = '\"'\n","CSV_INFER_SCHEMA = True\n","\n","# Where to drop extracted CSVs from ZIPs (within Lakehouse Files area)\n","TMP_EXTRACT_DIR = \"Files/RAW/_tmp_zip_extract\"\n","\n","# Object metadata\n","DEFAULT_SCHEMA = f\"{SRCDB_NAME.lower()}silver\"\n","DATE_FORMAT = \"yyyy-MM-dd_HH:mm:ss.S\"\n","\n","# Watermark\n","RUN_DATETIME_UTC = datetime.now(timezone.utc)   # notebook run instant\n","RUN_ISO = RUN_DATETIME_UTC.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n","\n","# Delta names\n","DELTA_TABLE_NAME = \"stage_ObjectList\"           # lakehouse Delta table name\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a72974ad-8518-4d64-9ff4-6c5902c04ec5"},{"cell_type":"markdown","source":["## 2) Imports & helpers"],"metadata":{},"id":"86fb9cfe-ce12-4aec-a0b2-c51e395b3e41"},{"cell_type":"code","source":["\n","import os, io, json, uuid, re, zipfile\n","from typing import List, Dict\n","from notebookutils import mssparkutils as msu\n","\n","from pyspark.sql import functions as F, types as T\n","\n","def log(msg: str):\n","    from datetime import datetime as _dt\n","    print(f\"[{_dt.utcnow().isoformat()}Z] {msg}\")\n","\n","def is_csv(path: str) -> bool:\n","    return path.lower().endswith(\".csv\")\n","\n","def is_zip(path: str) -> bool:\n","    return path.lower().endswith(\".zip\")\n","\n","def derive_object_name(file_name: str) -> str:\n","    name = os.path.basename(file_name)\n","    if name.lower().endswith(\".csv\"):\n","        name = name[:-4]\n","        name = re.sub(r'_?\\d{11,14}(?:\\.\\d+)?$', '', name) # added by Shuo to handle mini batches\n","        name = re.sub(r'\\d{12,14}(?:\\.\\d+)?$', '', name) # 2 different scenarios\n","    if name.startswith(\"file_\") and \"-batch\" in name:\n","        name = name.replace(\"file_\", \"\").split(\"-batch\")[0]\n","    return name\n","\n","def read_csv_with_spark(csv_path: str):\n","    reader = (spark.read\n","              .option(\"header\", str(CSV_HEADER).lower())\n","              .option(\"inferSchema\", str(CSV_INFER_SCHEMA).lower())\n","              .option(\"quote\", CSV_QUOTE)\n","              .option(\"sep\", CSV_DELIMITER))\n","    return reader.csv(csv_path)\n","\n","def schema_to_json(df_schema) -> List[Dict[str,str]]:\n","    items = []\n","    for f in df_schema.fields:\n","        items.append({\"name\": f.name, \"type\": f.dataType.simpleString()})\n","    return items\n","\n","def to_ddl(schema_json: List[Dict[str, str]]) -> str:\n","    cols = [f\"`{c['name']}` {c['type']}\" for c in schema_json]\n","    return \", \".join(cols)\n","\n","def extract_zip_to_tmp(zip_path: str, tmp_root: str) -> List[str]:\n","    \"\"\"\n","    Extract *.csv members from a ZIP stored in ABFSS to a temp ABFSS folder.\n","    Read ZIP via Spark (binaryFiles). Write CSVs in chunks:\n","      - prefer mssparkutils.fs.open('wb') if available\n","      - else use Hadoop FileSystem create() stream\n","    Returns the ABFSS paths of the extracted CSVs.\n","    \"\"\"\n","    import io, os, re, zipfile\n","\n","    CHUNK = 4 * 1024 * 1024  # 4MB chunks to avoid Py4J large-array issues\n","\n","    # --- small helpers kept local for drop-in use ---\n","\n","    def _read_zip_bytes(p: str) -> bytes:\n","        # Spark driver read (PortableDataStream or raw bytes)\n","        rdd = spark.sparkContext.binaryFiles(p, minPartitions=1)\n","        items = rdd.take(1)\n","        if not items:\n","            raise RuntimeError(f\"Could not read ZIP: {p}\")\n","        ds = items[0][1]\n","        try:\n","            return ds.read()          # PortableDataStream\n","        except AttributeError:\n","            return ds                 # already bytes\n","\n","    def _write_bytes_stream(dest: str, src_fileobj):\n","        \"\"\"\n","        Stream bytes from a Python file-like (src_fileobj) to ABFSS at dest.\n","        Uses mssparkutils.fs.open('wb') if present; else Hadoop FS OutputStream.\n","        \"\"\"\n","        if msu is not None and hasattr(msu.fs, \"open\"):\n","            # mssparkutils binary write\n","            parent = dest.rsplit(\"/\", 1)[0]\n","            try:\n","                msu.fs.mkdirs(parent)\n","            except Exception:\n","                pass\n","            with msu.fs.open(dest, 'wb') as out:\n","                while True:\n","                    chunk = src_fileobj.read(CHUNK)\n","                    if not chunk:\n","                        break\n","                    out.write(chunk)\n","            return\n","\n","        # Hadoop FS fallback (binary-safe)\n","        jvm = spark.sparkContext._jvm\n","        jsc = spark.sparkContext._jsc\n","        hconf = jsc.hadoopConfiguration()\n","        jpath = jvm.org.apache.hadoop.fs.Path(dest)\n","        fs = jpath.getFileSystem(hconf)\n","        parent = jpath.getParent()\n","        if parent is not None and not fs.exists(parent):\n","            fs.mkdirs(parent)\n","        out = fs.create(jpath, True)  # overwrite\n","        try:\n","            while True:\n","                chunk = src_fileobj.read(CHUNK)\n","                if not chunk:\n","                    break\n","                # pass small chunks to Py4J to avoid NegativeArraySizeException\n","                out.write(bytearray(chunk))\n","            out.hflush()\n","            out.hsync()\n","        finally:\n","            out.close()\n","\n","    # --- actual extraction logic ---\n","    tmp_root = tmp_root.rstrip(\"/\")\n","    zip_bytes = _read_zip_bytes(zip_path)\n","\n","    csv_paths: List[str] = []\n","    zip_base = os.path.splitext(os.path.basename(zip_path))[0]\n","\n","    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as z:\n","        for info in z.infolist():\n","            name = str(info.filename)\n","            is_dir = info.is_dir() if hasattr(info, \"is_dir\") else name.endswith((\"/\", \"\\\\\"))\n","            if is_dir or not name.lower().endswith(\".csv\"):\n","                continue\n","\n","            # sanitize inner path\n","            rel = name.replace(\"\\\\\", \"/\")\n","            rel = re.sub(r\"[\\r\\n]\", \"\", rel).replace(\"..\", \"\").strip(\"/\")\n","\n","            dest_path = f\"{tmp_root}/{zip_base}/{rel}\"\n","\n","            # stream from ZIP member to ABFSS in chunks\n","            with z.open(info, \"r\") as src:\n","                _write_bytes_stream(dest_path, src)\n","\n","            csv_paths.append(dest_path)\n","\n","    return csv_paths"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed270efd-d289-4f1e-a9b7-b8690266b4b0"},{"cell_type":"markdown","source":["## 3) Discover source files"],"metadata":{},"id":"206d8834-42f7-4619-9a89-572da610f7e7"},{"cell_type":"code","source":["# 1) Load ALL files in the folder (no wildcard, no recursion)\n","df_all = (spark.read.format(\"binaryFile\")\n","          .option(\"recursiveFileLookup\", \"false\")\n","          .load(RAW_PATH)                      # <-- loads every file in RAW/ERP\n","          .select(\"path\", \"modificationTime\", \"length\"))\n","\n","# 2) Case-insensitive filter for CSV/ZIP\n","lower_path = F.lower(F.col(\"path\"))\n","df_files = (df_all\n","            .where(lower_path.rlike(r\"\\.(csv|zip)$\"))\n","            .withColumn(\"ftype\", F.when(lower_path.endswith(\".csv\"), F.lit(\"csv\"))\n","                                   .when(lower_path.endswith(\".zip\"), F.lit(\"zip\"))\n","                                   .otherwise(F.lit(\"other\"))))\n","\n","count_candidates = df_files.count()\n","print(f\"Discovered {count_candidates} CSV/ZIP files in {RAW_PATH}\")\n","display(df_files.orderBy(\"path\").limit(200))\n","\n","# 3) Hand off to your scanning step\n","all_files = [(r[\"path\"], r[\"ftype\"]) for r in df_files.select(\"path\",\"ftype\").collect()]"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5217ee3d-3eb5-41b7-bd04-aeef302df66c"},{"cell_type":"markdown","source":["## 4) Scan rows and schemas"],"metadata":{},"id":"f6561b1a-56e1-4019-b2fd-e23abbd6d0d9"},{"cell_type":"code","source":["# --- threaded ingestion of CSVs & ZIPs ---\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import threading, os\n","\n","objects: Dict[str, Dict] = {}\n","objects_lock = threading.Lock()\n","\n","# If you want *true* overlap of Spark jobs, enable FAIR scheduling on your cluster.\n","# Example (works if the cluster is configured for FAIR):\n","# spark.conf.set(\"spark.scheduler.mode\", \"FAIR\")\n","# spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"ingest\")\n","\n","def _merge_object(obj: str, rec: Dict, extra_source: str | None = None):\n","    rec = dict(rec)  # shallow copy\n","    sp = rec.get('source_paths')\n","    if isinstance(sp, str):\n","        sp = [sp]\n","    rec['source_paths'] = list(sp or [])\n","    if extra_source:\n","        rec['source_paths'].append(extra_source)   # add here once\n","\n","    with objects_lock:\n","        if obj not in objects:\n","            objects[obj] = rec\n","            return\n","\n","        old = objects[obj]\n","        if isinstance(old.get('source_paths'), str):\n","            old['source_paths'] = [old['source_paths']]\n","\n","        old['row_count'] = max(old.get('row_count', 0), rec.get('row_count', 0))\n","\n","        if len(rec['schema_json']) != len(old['schema_json']):\n","            old['schema_json'] = rec['schema_json']\n","            old['schema_ddl']  = rec['schema_ddl']\n","\n","        # dedupe via the loop only — no extra append afterwards\n","        for p in rec['source_paths']:\n","            if p not in old['source_paths']:\n","                old['source_paths'].append(p)\n","                \n","def _handle_csv(path: str):\n","    try:\n","        df = read_csv_with_spark(path)\n","        cnt = df.count()\n","        sch_json = schema_to_json(df.schema)\n","        obj = derive_object_name(path)\n","        out_dir = f\"{BRONZE_PATH}/{obj}/{RUNDATE}/\"\n","\n","        rec = {\n","            'source_paths': [out_dir],  # keep your original semantics but normalized to list\n","            'row_count': cnt,\n","            'schema_json': sch_json,\n","            'schema_ddl': to_ddl(sch_json),\n","        }\n","\n","        # Copy csv to bronze\n","        (df.write\n","            .mode(\"append\")\n","            .option(\"header\", \"true\")\n","            .option(\"delimiter\", \",\")\n","            .option(\"quote\", '\"')\n","            .option(\"escape\", '\"')\n","            .option(\"nullValue\", \"\")\n","            .csv(out_dir)\n","        )\n","\n","        _merge_object(obj, rec)\n","        log(f\"🧾 CSV processed ✅ \\n {path} → rows={cnt} • cols={len(sch_json)}\")\n","    except Exception as e:\n","        log(f\"❌ ERROR reading CSV | {path} | {e}\")\n","\n","def _handle_zip(path: str):\n","    try:\n","        csv_paths = extract_zip_to_tmp(path, TMP_EXTRACT_DIR)\n","        log(f\"🗜️ ZIP extracted ✅ | {path} → {len(csv_paths)} file(s)\")\n","    except Exception as e:\n","        log(f\"💥 ERROR extracting ZIP | {path} | {e}\")\n","        return\n","\n","    # Process the inner CSVs (still on this worker thread).\n","    # If you want *each* inner CSV parallelized too, you can submit them to the same executor.\n","    for cpath in csv_paths:\n","        try:\n","            df = read_csv_with_spark(cpath)\n","            cnt = df.count()\n","            sch_json = schema_to_json(df.schema)\n","            obj = derive_object_name(cpath)\n","            out_dir = f\"{BRONZE_PATH}/{obj}/{RUNDATE}/\"\n","\n","            rec = {\n","                'source_paths': [out_dir],\n","                'row_count': cnt,\n","                'schema_json': sch_json,\n","                'schema_ddl': to_ddl(sch_json),\n","            }\n","\n","            (df.write\n","                .mode(\"append\")\n","                .option(\"header\", \"true\")\n","                .option(\"delimiter\", \",\")\n","                .option(\"quote\", '\"')\n","                .option(\"escape\", '\"')\n","                .option(\"nullValue\", \"\")\n","                .csv(out_dir)\n","            )\n","\n","            # Your original loop appended the inner CSV path into source_paths; preserve that.\n","            _merge_object(obj, rec, extra_source=cpath)\n","            log(f\"📦 ZIP member processed ✅ \\n {cpath} → rows={cnt} • cols={len(sch_json)}\")\n","        except Exception as ie:\n","            log(f\"⚠️ ERROR reading inner CSV | {cpath} | {ie}\")\n","\n","def _handle_item(item):\n","    path, ftype = item\n","    if ftype == 'csv':\n","        _handle_csv(path)\n","    elif ftype == 'zip':\n","        _handle_zip(path)\n","    else:\n","        log(f\"ℹ️ Skipping unsupported type: {ftype} | {path}\")\n","\n","# --- fan out work across threads ---\n","cpu_cap = (os.cpu_count() or 4) * 2\n","max_workers = max(1, min(int(PARALLELISM), cpu_cap))  # cap by CPU to be safe\n","with ThreadPoolExecutor(max_workers=max_workers) as pool:\n","    futures = [pool.submit(_handle_item, it) for it in all_files]\n","    for _ in as_completed(futures):\n","        pass  # just drain exceptions if any; errors are already logged inside handlers\n","\n","log(f\"📊 Total objects discovered: {len(objects)}\")\n","print(objects)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"585d9ec1-2810-4321-8e6f-0aea7445392d"},{"cell_type":"code","source":["# Drop zip extract temp dir\n","try:\n","    mssparkutils.fs.rm(TMP_EXTRACT_DIR.rstrip(\"/\"), True)\n","except:\n","    pass"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b845023d-2619-43e5-8afb-2f406e2afa42"},{"cell_type":"markdown","source":["### Archive batch to /Archive"],"metadata":{},"id":"ebdd7f5d-bb58-457a-994d-c6134f954dd8"},{"cell_type":"code","source":["# Archive files in RAW to CONTROL_PATH\n","mssparkutils.fs.cp(RAW_PATH.rstrip('/')+'/', ARCHIVE_PATH.rstrip('/')+'/', True)\n","\n","for item in mssparkutils.fs.ls(RAW_PATH):\n","    if not item.isDir:                # files only\n","         mssparkutils.fs.rm(item.path, False)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f17353d2-eb71-456e-a727-df32bcf27b15"},{"cell_type":"markdown","source":["## 5) Build `stage.ObjectList` rows (with watermark RunDateUtc)"],"metadata":{},"id":"e6c2e16d-787d-40be-a871-3336f526a17d"},{"cell_type":"code","source":["from pyspark.sql import types as T\n","\n","# Ensure a NOT NULL value for DatabaseName (it’s required in SQL)\n","db_name = globals().get(\"jdbc_database\", \"\")   # falls back to '' if not set\n","\n","stage_rows = []\n","for obj, meta in objects.items():\n","    stage_rows.append({\n","        \"ObjectSchema\":          DEFAULT_SCHEMA,               # nvarchar(255) NULL\n","        \"ObjectName\":            obj,                          # nvarchar(255) NOT NULL\n","        \"EstimatedRowCount\":     int(meta['row_count']),       # bigint NULL\n","        \"WaterMarkColumn\":       \"\",                           # nvarchar(255) NULL\n","        \"WaterMarkColumnValue\":  RUN_ISO,                      # nvarchar(255) NULL\n","        \"WaterMarkType\":         \"datetime\",                   # varchar(50)  NULL\n","        \"TableName\":             obj,                          # nvarchar(255) NOT NULL\n","        \"CandidateKey\":          None,                         # nvarchar(1000) NULL\n","        \"DatabaseName\":          SRCDB_NAME,                      # nvarchar(255) NOT NULL\n","    })\n","\n","stage_schema = T.StructType([\n","    T.StructField(\"ObjectSchema\",         T.StringType(),    True),\n","    T.StructField(\"ObjectName\",           T.StringType(),    False),\n","    T.StructField(\"EstimatedRowCount\",    T.LongType(),      True),\n","    T.StructField(\"WaterMarkColumn\",      T.StringType(),    True),\n","    T.StructField(\"WaterMarkColumnValue\", T.StringType(),    True),\n","    T.StructField(\"WaterMarkType\",        T.StringType(),    True),\n","    T.StructField(\"TableName\",            T.StringType(),    False),\n","    T.StructField(\"CandidateKey\",         T.StringType(),    True),\n","    T.StructField(\"DatabaseName\",         T.StringType(),    False),\n","])\n","\n","stage_df = spark.createDataFrame(stage_rows, schema=stage_schema)\n","\n","\n","display(stage_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c30afe53-7022-429b-a2b0-00eb775bb01a"},{"cell_type":"markdown","source":["## 6) Log to ControlDb\n"],"metadata":{},"id":"418d5e59-8ada-4cc4-86a3-02c9799eb3c6"},{"cell_type":"code","source":["# --- Azure Key Vault Configuration ---\n","key_vault_uri = \"https://dg-fabric-kv-dev.vault.azure.net/\"\n","secret_name = \"controlDBConnection\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15d40ce1-9ec0-4ae8-9597-114450e69dca"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from urllib.parse import urlparse, parse_qs\n","\n","\n","# --- Function to parse JDBC URL ---\n","def parse_jdbc_url(jdbc_url_string):\n","    \"\"\"\n","    Parses a JDBC URL string and returns a dictionary of its components.\n","    \n","    Args:\n","        jdbc_url_string (str): The full JDBC connection string.\n","        \n","    Returns:\n","        dict: A dictionary containing the parsed key-value pairs.\n","    \"\"\"\n","    # The first part is the protocol and the server/port.\n","    # The remaining parts are key-value pairs separated by semicolons.\n","    parts = jdbc_url_string.split(';')\n","    \n","    # Extract the server and port from the first part\n","    protocol_and_host = parts[0].split(\"://\")[1]\n","    server_port_part = protocol_and_host.split(\":\")\n","    \n","    params = {}\n","    params['server'] = server_port_part[0]\n","    # Check if a port is present, otherwise use default\n","    if len(server_port_part) > 1:\n","        params['port'] = server_port_part[1]\n","    else:\n","        params['port'] = '1433' # Default SQL Server port\n","    \n","    # Parse the remaining key-value pairs\n","    for part in parts[1:]:\n","        if '=' in part:\n","            key, value = part.split('=', 1)\n","            params[key.strip()] = value.strip()\n","    return params\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"095b16f0-5000-4dee-9d1f-ae1a9598c78d"},{"cell_type":"code","source":["# --- Retrieve the secret from Azure Key Vault ---\n","print(f\"Retrieving secret '{secret_name}' from Azure Key Vault...\")\n","try:  \n","    jdbc_connection_string_param = notebookutils.credentials.getSecret(key_vault_uri, secret_name)\n","    print(\"Secret retrieved successfully.\")\n","except Exception as e:\n","    print(f\"An error occurred while retrieving the secret from Key Vault: {e}\")\n","    jdbc_connection_string_param = None\n","    \n","if jdbc_connection_string_param:\n","\n","    connection_params = parse_jdbc_url(jdbc_connection_string_param)\n","\n","    jdbc_server = connection_params.get('server')\n","    jdbc_database = connection_params.get('database')\n","    jdbc_user = connection_params.get('user')\n","    jdbc_password = connection_params.get('password')\n","\n","    jdbc_url = (\n","        f\"jdbc:sqlserver://{jdbc_server}:1433;\"\n","        f\"database={jdbc_database};\"\n","        \"encrypt=true;\"\n","        \"trustServerCertificate=false;\"\n","        \"hostNameInCertificate=*.database.windows.net;\"\n","        \"loginTimeout=30;\"\n","    )\n","\n","\n","    try:    \n","        # Overwrite rows in stage.ObjectList (keeps table definition, PKs, indexes)\n","        (stage_df.write\n","                .format(\"jdbc\")\n","                .mode(\"append\")\n","                #.option(\"truncate\", \"true\")   # TRUNCATE + INSERT\n","                .option(\"url\", jdbc_url)\n","                .option(\"dbtable\", \"stage.ObjectList\")   # or \"[stage].[ObjectList]\"\n","                .option(\"user\", jdbc_user)\n","                .option(\"password\", jdbc_password)\n","                .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n","                .save())\n","\n","    except Exception as e:\n","        print(f\"An error occurred during Spark operation: {e}\")\n","        \n","else:\n","    print(\"JDBC connection string could not be retrieved. Aborting Spark session.\")\n","\n","\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"run_control":{"frozen":false},"editable":true,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9cdbe907-c962-43ad-80c8-db2701898ef0"},{"cell_type":"code","source":["\n","\n","# --- Retrieve the secret from Azure Key Vault ---\n","print(f\"Retrieving secret '{secret_name}' from Azure Key Vault...\")\n","try:  \n","    jdbc_connection_string_param = notebookutils.credentials.getSecret(key_vault_uri, secret_name)\n","    print(\"Secret retrieved successfully.\")\n","except Exception as e:\n","    print(f\"An error occurred while retrieving the secret from Key Vault: {e}\")\n","    jdbc_connection_string_param = None\n","    \n","if jdbc_connection_string_param:\n","\n","    connection_params = parse_jdbc_url(jdbc_connection_string_param)\n","\n","    jdbc_server = connection_params.get('server')\n","    jdbc_database = connection_params.get('database')\n","    jdbc_user = connection_params.get('user')\n","    jdbc_password = connection_params.get('password')\n","\n","    jdbc_url = (\n","        f\"jdbc:sqlserver://{jdbc_server}:1433;\"\n","        f\"database={jdbc_database};\"\n","        \"encrypt=true;\"\n","        \"trustServerCertificate=false;\"\n","        \"hostNameInCertificate=*.database.windows.net;\"\n","        \"loginTimeout=30;\"\n","    )\n","\n","    # The SQL query to be executed\n","    query_to_run = \"SELECT TOP (1000) * FROM [dbo].[ObjectList]\" \n","\n","\n","    try:\n","        df = spark.read \\\n","          .format(\"jdbc\") \\\n","          .option(\"url\", jdbc_url) \\\n","          .option(\"dbtable\", f\"({query_to_run}) AS T\") \\\n","          .option(\"user\", jdbc_user) \\\n","          .option(\"password\", jdbc_password) \\\n","          .load()\n","\n","        # Show the resulting DataFrame\n","        print(\"\\nDataFrame successfully loaded from Azure SQL database:\")\n","        display(df)\n","        \n","\n","    except Exception as e:\n","        print(f\"An error occurred during Spark operation: {e}\")\n","        \n","else:\n","    print(\"JDBC connection string could not be retrieved. Aborting Spark session.\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13ac4696-3a6e-4f3e-9088-77a6143e1dfc"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0857a21-e8b2-4e19-bfc7-e766f192c767"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"a786a911-fcdb-4d4b-bb6d-4f4688d71ab9","default_lakehouse_name":"BronzeSilver","default_lakehouse_workspace_id":"8d19302d-6ac0-4b8c-bd3a-4d3a93935c86","known_lakehouses":[{"id":"a786a911-fcdb-4d4b-bb6d-4f4688d71ab9"}]}}},"nbformat":4,"nbformat_minor":5}